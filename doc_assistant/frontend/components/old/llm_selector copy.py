import streamlit as st
from pathlib import Path
from dsrag.llm import OpenAIChatAPI, AnthropicChatAPI, LLM

class LLMSelector:
    LLM_CONFIGS = {
        "OpenAI": {
            "class": OpenAIChatAPI,
            "models": [
                {
                    "id": "gpt-4-0125-preview",
                    "name": "GPT-4 Turbo Preview",
                    "description": "Version la plus r√©cente et performante"
                },
                {
                    "id": "gpt-4-turbo-preview",
                    "name": "GPT-4 Turbo",
                    "description": "Excellent rapport performance/co√ªt"
                },
                {
                    "id": "gpt-4",
                    "name": "GPT-4",
                    "description": "Mod√®le stable et fiable"
                },
                {
                    "id": "gpt-3.5-turbo",
                    "name": "GPT-3.5 Turbo",
                    "description": "Rapide et √©conomique"
                }
            ],
            "default_model": "gpt-4-turbo-preview",
            "default_temp": 0.2,
            "default_max_tokens": 1000,
            "color": "#74AA9C"
        },
        "Anthropic": {
            "class": AnthropicChatAPI,
            "models": [
                {
                    "id": "claude-3-opus-20240229",
                    "name": "Claude 3 Opus",
                    "description": "Le plus performant des mod√®les Claude"
                },
                {
                    "id": "claude-3-sonnet-20240229",
                    "name": "Claude 3 Sonnet",
                    "description": "Bon compromis performance/rapidit√©"
                },
                {
                    "id": "claude-3-haiku-20240307",
                    "name": "Claude 3 Haiku",
                    "description": "Version rapide et √©conomique"
                }
            ],
            "default_model": "claude-3-sonnet-20240229",
            "default_temp": 0.2,
            "default_max_tokens": 1000,
            "color": "#000000"
        }
    }

    def __init__(self):
        # Initialisation des √©tats de session
        if 'llm_provider' not in st.session_state:
            st.session_state.llm_provider = "OpenAI"
        if 'llm_model' not in st.session_state:
            st.session_state.llm_model = self.LLM_CONFIGS["OpenAI"]["default_model"]
        if 'llm_temperature' not in st.session_state:
            st.session_state.llm_temperature = self.LLM_CONFIGS["OpenAI"]["default_temp"]
        if 'llm_max_tokens' not in st.session_state:
            st.session_state.llm_max_tokens = self.LLM_CONFIGS["OpenAI"]["default_max_tokens"]

    def _render_provider_selection(self):
        """Affiche les boutons de s√©lection du provider"""
        st.write("#### S√©lection du provider")
        cols = st.columns(len(self.LLM_CONFIGS))
        
        for idx, (provider, config) in enumerate(self.LLM_CONFIGS.items()):
            with cols[idx]:
                selected = st.session_state.llm_provider == provider
                if st.button(
                    provider,
                    key=f"provider_{provider}",
                    type="primary" if selected else "secondary",
                    use_container_width=True
                ):
                    st.session_state.llm_provider = provider
                    st.session_state.llm_model = config["default_model"]
                    st.rerun()

    def _render_model_selection(self):
        """Affiche les cartes de s√©lection des mod√®les"""
        current_config = self.LLM_CONFIGS[st.session_state.llm_provider]
        st.write("#### S√©lection du mod√®le")
        
        for model in current_config["models"]:
            col1, col2 = st.columns([5, 1])
            with col1:
                st.write(f"**{model['name']}**")
                st.write(model['description'])
            with col2:
                selected = st.session_state.llm_model == model["id"]
                if st.button(
                    "S√©lectionner" if not selected else "‚úì",
                    key=f"model_{model['id']}",
                    type="primary" if selected else "secondary",
                ):
                    st.session_state.llm_model = model["id"]
                    st.rerun()
            st.write("---")

    def render(self) -> LLM:
        st.markdown("### ü§ñ Configuration du mod√®le")
        
        # S√©lection du provider
        self._render_provider_selection()
        st.write("---")
        
        # S√©lection du mod√®le
        self._render_model_selection()

        # Configuration avanc√©e
        with st.expander("‚öôÔ∏è Configuration avanc√©e"):
            col1, col2 = st.columns(2)
            with col1:
                st.session_state.llm_temperature = st.slider(
                    "Temp√©rature",
                    0.0, 1.0, 
                    st.session_state.llm_temperature,
                    0.1,
                    help="Contr√¥le la cr√©ativit√© des r√©ponses"
                )
            with col2:
                st.session_state.llm_max_tokens = st.number_input(
                    "Tokens Maximum",
                    100, 4000,
                    st.session_state.llm_max_tokens,
                    100
                )

        # Cr√©ation de l'instance LLM
        current_config = self.LLM_CONFIGS[st.session_state.llm_provider]
        llm_class = current_config["class"]
        return llm_class(
            model=st.session_state.llm_model,
            temperature=st.session_state.llm_temperature,
            max_tokens=st.session_state.llm_max_tokens
        )